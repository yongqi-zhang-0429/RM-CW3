define({ entries : {
    "10.1145": {
        "abstract": "Depression is one of the most common mood disorders. Technology has the potential to assist in screening and treating people with depression by robustly modeling and tracking the complex behavioral cues associated with the disorder (e.g., speech, language, facial expressions, head movement, body language). Similarly, robust affect recognition is another challenge which stands to benefit from modeling such cues. The Audio/Visual Emotion Challenge (AVEC) aims toward understanding the two phenomena and modeling their correlation with observable cues across several modalities. In this paper, we use multimodal signal processing methodologies to address the two problems using data from human-computer interactions. We develop separate systems for predicting depression levels and affective dimensions, experimenting with several methods for combining the multimodal information. The proposed depression prediction system uses a feature selection approach based on audio, visual, and linguistic cues to predict depression scores for each session. Similarly, we use multiple systems trained on audio and visual cues to predict the affective dimensions in continuous-time. Our affect recognition system accounts for context during the frame-wise inference and performs a linear fusion of outcomes from the audio-visual systems. For both problems, our proposed systems outperform the video-feature based baseline systems. As part of this work, we analyze the role played by each modality in predicting the target variable and provide analytical insights.",
        "address": "New York, NY, USA",
        "author": "Gupta, Rahul and Malandrakis, Nikolaos and Xiao, Bo and Guha, Tanaya and Van Segbroeck, Maarten and Black, Matthew and Potamianos, Alexandros and Narayanan, Shrikanth",
        "booktitle": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge",
        "doi": "10.1145/2661806.2661810",
        "isbn": "9781450331197",
        "keywords": "valence, multimodal signal processing, fusion, dominance, depression, behavioral signal processing (bsp), arousal, multimodal",
        "location": "Orlando, Florida, USA",
        "numpages": "8",
        "pages": "33\u201340",
        "publisher": "Association for Computing Machinery",
        "series": "AVEC '14",
        "title": "Multimodal Prediction of Affective Dimensions and Depression in Human-Computer Interactions",
        "type": "inproceedings",
        "url": "https://doi.org/10.1145/2661806.2661810",
        "year": "2014"
    },
    "10446224": {
        "abstract": "Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset.",
        "author": "Wagner, Dominik and Churchill, Alexander and Sigtia, Siddharth and Georgiou, Panayiotis and Mirsamadi, Matt and Mishra, Aarshee and Marchi, Erik",
        "booktitle": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
        "doi": "10.1109/ICASSP48485.2024.10446224",
        "issn": "2379-190X",
        "keywords": "LLM, multimodal Voice activity detection, Training, Adaptation models, Virtual assistants, Acoustics, Decoding, Task analysis, device-directed speech detection, large language model, multimodal, conditional generation",
        "month": "April",
        "number": "",
        "pages": "10451-10455",
        "title": "A Multimodal Approach to Device-Directed Speech Detection with Large Language Models",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2024"
    },
    "3610977.3634966": {
        "abstract": "Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n",
        "address": "New York, NY, USA",
        "author": "Kim, Callie Y. and Lee, Christine P. and Mutlu, Bilge",
        "booktitle": "Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction",
        "doi": "10.1145/3610977.3634966",
        "isbn": "9798400703225",
        "keywords": "human-robot interaction, large language models, social robots, LLM",
        "location": "<conf-loc>, <city>Boulder</city>, <state>CO</state>, <country>USA</country>, </conf-loc>",
        "numpages": "10",
        "pages": "371\u2013380",
        "publisher": "Association for Computing Machinery",
        "series": "HRI '24",
        "title": "Understanding Large-Language Model (LLM)-powered Human-Robot Interaction",
        "type": "inproceedings",
        "url": "https://doi.org/10.1145/3610977.3634966",
        "year": "2024"
    },
    "9428016": {
        "abstract": "In human\u2013robot shared manufacturing contexts, product parts or tools hand-over between the robot and the human is an important collaborative task. Facilitating the robot to figure out and predict human hand-over intentions correctly to improve the task efficiency in human\u2013robot collaboration is therefore a necessary issue to be addressed. In this study, a teaching-learning-prediction (TLP) framework is proposed for the robot to learn from its human partner\u2019s multimodal demonstrations and predict human hand-over intentions. In this approach, the robot can be programmed by the human through demonstrations utilizing natural language and wearable sensors according to task requirements and the human\u2019s working preferences. Then the robot learns from human hand-over demonstrations online via extreme learning machine (ELM) algorithms to update its cognition capacity, allowing the robot to use its learned policy to predict human intentions actively and assist its human companion in hand-over tasks. Experimental results and evaluations suggest that the human may program the robot easily by the proposed approach when the task changes, as the robot can effectively predict hand-over intentions with competitive accuracy to complete the hand-over tasks. Note to Practitioners\u2014This article is motivated by human\u2013robot hand-over problems in smart manufacturing contexts. Product parts or tools delivery in worker\u2013robot partnerships is an important collaborative task. We develop a teaching-learning-prediction (TLP) framework for the robot to learn from its human partner\u2019s multimodal demonstrations and predict human hand-over intentions. The robot can be taught by human through natural language and wearable sensing information. The extreme learning machine (ELM) approach is employed for the robot to build its cognition capacity to predict human intentions actively and assist its human companion in hand-over tasks. We demonstrate that the proposed approach presents distinct and effective advantages to facilitate human\u2013robot hand-over tasks in collaborative manufacturing contexts.",
        "author": "Wang, Weitian and Li, Rui and Chen, Yi and Sun, Yi and Jia, Yunyi",
        "doi": "10.1109/TASE.2021.3074873",
        "issn": "1558-3783",
        "journal": "IEEE Transactions on Automation Science and Engineering",
        "keywords": "Robots, Task analysis, Robot sensing systems, Collaboration, Education, Cognition, Tool, Extreme learning machine (ELM), human\u2013robot hand-over, intention prediction, learning from demonstrations, natural language, wearable sensors, multimodal",
        "month": "July",
        "number": "3",
        "pages": "2339-2353",
        "title": "Predicting Human Intentions in Human\u2013Robot Hand-Over Tasks Through Multimodal Learning",
        "type": "ARTICLE",
        "volume": "19",
        "year": "2022"
    },
    "Ali2024ComparingAT": {
        "abstract": "Intention-based Human-Robot Interaction (HRI) systems allow robots to perceive and interpret user actions to proactively interact with humans and adapt to their behavior. Therefore, intention prediction is pivotal in creating a natural interactive collaboration between humans and robots. In this paper, we examine the use of Large Language Models (LLMs) for inferring human intention during a collaborative object categorization task with a physical robot. We introduce a hierarchical approach for interpreting user non-verbal cues, like hand gestures, body poses, and facial expressions and combining them with environment states and user verbal cues captured using an existing Automatic Speech Recognition (ASR) system. Our evaluation demonstrates the potential of LLMs to interpret non-verbal cues and to combine them with their context-understanding capabilities and real-world knowledge to support intention prediction during human-robot interaction.",
        "author": "Hassan Ali and Philipp Allgeuer and Stefan Wermter",
        "doi": "https://doi.org/10.48550/arXiv.2404.08424",
        "keywords": "LLM, intention prediction",
        "title": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction in an Object Categorization Task",
        "type": "inproceedings",
        "url": "https://api.semanticscholar.org/CorpusID:269137567",
        "year": "2024"
    },
    "Driess2023PaLMEAE": {
        "abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
        "author": "Danny Driess and F. Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Ho Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Peter R. Florence",
        "booktitle": "International Conference on Machine Learning",
        "doi": "https://doi.org/10.48550/arXiv.2303.03378",
        "keywords": "LLM",
        "title": "PaLM-E: An Embodied Multimodal Language Model",
        "type": "inproceedings",
        "url": "https://api.semanticscholar.org/CorpusID:257364842",
        "year": "2023"
    },
    "Lee2024ImprovingDA": {
        "abstract": "We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI) multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.",
        "author": "Dong Won Lee and Hae Won Park and Yoon Kim and Cynthia Breazeal and Louis-Philippe Morency",
        "doi": "https://doi.org/10.48550/arXiv.2403.11330",
        "journal": "ArXiv",
        "keywords": "LLM, multimodal",
        "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback",
        "type": "article",
        "url": "https://api.semanticscholar.org/CorpusID:268513177",
        "volume": "abs/2403.11330",
        "year": "2024"
    },
    "Peng2024LCLLMEL": {
        "abstract": "To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as prompts for input into the LLM and employing a supervised fine-tuning technique to tailor the LLM specifically for our lane change prediction task. This allows us to utilize the LLM's powerful common sense reasoning abilities to understand complex interactive information, thereby improving the accuracy of long-term predictions. Furthermore, we incorporate explanatory requirements into the prompts in the inference stage. Therefore, our LC-LLM model not only can predict lane change intentions and trajectories but also provides explanations for its predictions, enhancing the interpretability. Extensive experiments on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can encode comprehensive interaction information for driving behavior understanding.",
        "author": "Mingxing Peng and Xusen Guo and Xianda Chen and Meixin Zhu and Kehua Chen and Hao Yang and Xuesong Wang and Yinhai Wang",
        "doi": "https://doi.org/10.48550/arXiv.2403.18344",
        "journal": "ArXiv",
        "keywords": "LLM",
        "title": "LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models",
        "type": "article",
        "url": "https://api.semanticscholar.org/CorpusID:268723960",
        "volume": "abs/2403.18344",
        "year": "2024"
    },
    "Wang2024LargeLM": {
        "abstract": "Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.",
        "author": "Jiaqi Wang and Zihao Wu and Yiwei Li and Hanqi Jiang and Peng Shu and Enze Shi and Huawen Hu and Chong-Yi Ma and Yi-Hsueh Liu and Xuhui Wang and Yincheng Yao and Xuan Liu and Huaqin Zhao and Zheng Liu and Haixing Dai and Lin Zhao and Bao Ge and Xiang Li and Tianming Liu and Shu Zhang",
        "doi": "https://doi.org/10.48550/arXiv.2401.04334",
        "journal": "ArXiv",
        "keywords": "LLM",
        "title": "Large Language Models for Robotics: Opportunities, Challenges, and Perspectives",
        "type": "article",
        "url": "https://api.semanticscholar.org/CorpusID:266899905",
        "volume": "abs/2401.04334",
        "year": "2024"
    },
    "Zheng2024LargeLM": {
        "abstract": "Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts -- Transportation Context Map (TC-Map), accompanied by corresponding text prompts. Through this approach, we obtained rich traffic context information from the LLM. By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7\\% LLM-augmented datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving.",
        "author": "Xiaoji Zheng and Lixiu Wu and Zhijie Yan and Yuanrong Tang and Hao Zhao and Chen Zhong and Bokui Chen and Jiangtao Gong",
        "doi": "https://doi.org/10.48550/arXiv.2403.11057",
        "journal": "ArXiv",
        "keywords": "LLM",
        "title": "Large Language Models Powered Context-aware Motion Prediction",
        "type": "article",
        "url": "https://api.semanticscholar.org/CorpusID:268513306",
        "volume": "abs/2403.11057",
        "year": "2024"
    }
}});